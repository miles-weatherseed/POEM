# Which features to include in input
dataset:
  tcr: NetTepi                   # NetTepi, PRIME2.0 or None
  length: true                   # Whether to include a sparse encoding of the peptide length
  mhci_sequence: pseudosequence  # Whether to use full sequence or pseudosequence, or None
  mhci_encoding: sparse          # How to encode the mhci_sequence, if at all
  randoms: true                  # Whether to include random/decoys in training data
  normalization: MinMax          # Whether and how to normalize input data (MinMax, Standard, None)

# Model configuration
model:
  hidden_layers: [256, 8]        # List defining the size of each hidden layer
  activation: relu               # Activation function for hidden layers
  output_activation: sigmoid     # Activation function for output layer
  dropout_rate: 0.0              # Dropout rate for regularization

# Training configuration
training:
  epochs: 200                     # Number of training epochs
  batch_size: 1                 # Batch size for training
  learning_rate: 0.01           # Learning rate for the optimizer
  optimizer: sgd                # Optimizer type (e.g., adam, sgd)
  loss_function: binary_crossentropy  # Loss function for classification tasks
  validation_split: 0.33          # Fraction of data to use for validation
  early_stopping:                # Early stopping configuration
    enabled: true
    patience: 10                  # Stop training if no improvement for this many epochs
    restore_best_weights: true

# Logging and saving configuration
logging:
  save_model: true               # Save the model after training
  save_path: models/mlp_model.h5  # Path to save the trained model