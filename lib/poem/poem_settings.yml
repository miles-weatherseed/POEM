# Which features to include in input
dataset:
  tcr_encoding: NetTepi          # TCR encoding strategy: NetTepi, PRIME2.0 or None
  length_encoding: true          # Boolean: whether to include a sparse encoding of the peptide length
  mhci_sequence: pseudosequence  # Whether to use full sequence or pseudosequence, or None
  mhci_encoding: sparse          # How to encode the mhci_sequence, if at all
  #randoms: true                 # Whether to include random/decoys in PRIME2.0 training data - maybe shouldn't be in YAML
  normalization: Standard          # Whether and how to normalize input data (MinMax, Standard, None)

# Model configuration
model:
  version: ensemble
  hidden_layers: [256, 8]        # List defining the size of each hidden layer
  activation: relu               # Activation function for hidden layers
  output_activation: sigmoid     # Activation function for output layer
  dropout_rate: 0.0              # Dropout rate for regularization

# Training configuration
training:
  kfolds: 10                    # how many folds to use for cross-validation
  stratified: true              # whether to use stratified K-folds
  random_seed: 42               # random seed to ensure reproducibility
  epochs: 200                   # Number of training epochs
  batch_size: 32                 # Batch size for training
  learning_rate: 0.01           # Learning rate for the optimizer
  optimizer: sgd                # Optimizer type (e.g., adam, sgd)
  loss_function: binary_crossentropy  # Loss function for classification tasks
  validation_split: 0.20          # Fraction of data to use for validation
  early_stopping:                # Early stopping configuration
    enabled: true
    patience: 10                  # Stop training if no improvement for this many epochs
    restore_best_weights: true

test:
  version: ensemble
  mlp_path: trained_models/mlp_model.h5
  mlp_scaler_path: trained_models/mlp_model_scaler.pkl
  lgbm_path: trained_models/LightGBM_POEM.pkl
  logreg_path: trained_models/LogReg.pkl
  logreg_scaler_path: trained_models/LogReg_scaler.pkl
  
# Logging and saving configuration
logging:
  save_model: true               # Save the model if training
  save_path: trained_models/mlp_model.h5  # Path to save/load the trained model